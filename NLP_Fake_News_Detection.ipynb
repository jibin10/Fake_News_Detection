{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Domain Specific Area\n",
    "\n",
    "The world's digital population has increased dramatically in the last decade. 'As of April 2022, there were 5 billion internet users worldwide, which is 63% of the global population. Of this total, 4.65 billion were social media users' [1 Joseph Johnson, 9 May 2022, statista.com]. These users receive an ample amount of information every day through different sources. It also raised the circulation of misinformation or fake news over the internet. Human publishers and a massive amount of bots are deployed to spread misinformation over social media, email, SMS or instant messengers. Twitter reports that 5% of Twitter accounts are bots. In the last decade, the influence of fake news has increased to a level that it affected elections, national referendums etc., according to news reports [2]. The bad actors aim to damage the reputation of persons, governments or organisations by spreading fake news. This project report is an effort to distinguish fake news from real news using natural language processing and text classification methods.\n",
    "\n",
    "The report is focused on digital news or information from various sources and tries to identify fake news information. We create a machine learning-based automated method to identify fake news, which is scalable to the general public in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the problem?\n",
    "\n",
    "Online fact-checkers and social media companies use advanced machine learning models to identify fake news and label social media posts. But these algorithms are not open source and may be influenced by the political bias of the corporations. 3rd party fact-checkers cannot independently verify every news article or social media post. Alternative news portals other than the major ones, instant messengers, SMS, email, and online ads are also used by malicious actors, and state agencies[3] to spread misinformation. Though big corporations can filter information on their platform,  it is still not enough to stop malicious actors from spreading fake news targeting end-users based on geolocation, demographic and ethnicity. \n",
    "\n",
    "The solution is equipping the end-user with a free, open-source and automated system, which uses the power of machine learning and deep learning to distinguish fake information. Though this report doesn't offer such a complete product, it is a first step towards the idea and delivers proof of concept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are the challeges?\n",
    "\n",
    "The basic challenge is to deal with the ambiguity in languages. Our speech or writing may be interpreted differently. Fake news is equally affecting every language, but making a solution for every language is not feasible. Another challenge is the approach we select to solve the problem. Rule-based methods are accurate but difficult to scale and maintain. Machine learning methods like classification are scalable, but the prediction may not be accurate for every dataset.\n",
    "\n",
    "Finding a balanced dataset for training the model is another challenge. Any bias in the training data will affect the performance of the model on unseen data. Other challenges are ethical concerns about using a dataset and laws specific to countries. Finding the best ML methods for classification and evaluation are important challenges to this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Objectives\n",
    "\n",
    "#### Why are you doing what you are doing?\n",
    "\n",
    "The goal of the project is to provide a proof of concept of a fake news detection solution for the public. Most of the solutions available in the market are middlewares deployed by social media companies, news media or fact-checkers. Social media companies like Twitter and Facebook may label misinformation or suppress its reach. But their algorithms are not open source, and nobody knows what actions they take to a particular social media post. Many news reports and whistleblowers came out against the political bias of these organisations and accused them of promoting or demoting certain groups or content [4][5]. The conventional news media is also not free from bias. Social media or mass media organisations hold the power to suppress information based on their political views. This project doesn't offer a solution to the media bias problem. But it tries to empower the user with technology and bring the analysis result to them to make their own decision. \n",
    "\n",
    "This solution uses advanced machine learning and deep learning methods to distinguish fake news/information from real information independent of sources. It is highly scalable as users can check virtually any information they receive (Social media posts, news, direct messages, emails, information from instant messengers). For example, if the user receives an SMS with a call to action link, our model will inform the user about the chances it is fake or not. It brings the model's prediction and evaluation matrics direct to the user so that they can decide about the information or news they received.\n",
    "\n",
    "#### Potential impact and contributions\n",
    "\n",
    "This is the first step toward building an AI-based automated fake information checker as a service to the end-user. A mobile and web application that can receive data as speech, text or text written on an image (e.g. screenshot of a social media post or SMS) and inform the user whether it is fake or not would be a unique solution in the market. \n",
    "\n",
    "Rather than just classifying the information as fake or not fake, if the user gets more details like the evaluation matrics, what is the accuracy of the prediction, what are the classification methods used etc., it will be more helpful to make a decision. Making the source code available to the public in a repository (open-source) and creating an open developer community like Linux will be good to maintain the public trust. Each change to the algorithm will be available to the public for verification. This approach will encourage the public to use the application without presumptions, and they will eventually provide valuable data to train the model further to improve its accuracy. It may lead the big corporations to provide more information on how they classify, suppress or promote content online, it would be a great impact on the industry.\n",
    "\n",
    "As the first step, this report provide a basic proof of concept of the greater plan.\n",
    "\n",
    "#### Out of scope\n",
    "\n",
    "- Web and mobile apps\n",
    "- AI Solution as a service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Dataset\n",
    "\n",
    "The quality of the dataset, reliability of the source, and its balance were the criteria for dataset selection. The 'Fake News Classification on WELFake Dataset' on Kaggle (a subsidiary of Google) is one of the weekly updated datasets. This dataset is published in the IEEE Transactions on Computational Social Systems. Considering these factors, I think this dataset is reliable for the NLP project.\n",
    "\n",
    "It has a total of 72,134 news articles, with 35,028 real and 37,106 fake news. The fake-news to real-news ratio is 51:49 and well balanced to train our model. The authors of the dataset collected the news from Kaggle, McIntire, Reuters, and BuzzFeed Political and merged it to form the 'WELFake Dataset'. It is a diverse news dataset, and it will somewhat prevent the over-fitting of machine learning models. \n",
    "\n",
    "The dataset's size is 245.09 MiB and is available in .csv format. It has four columns, the index, title, text, and label. Each row represents a news instance. The index column is the unique identifier for the rows, and the index is integers starting from zero. The title column contains the news titles in string format. Skimming through the dataset shows there are a few empty titles, and we can remove those rows during the preprocessing of data. The text column has more details on the news in string format, and the label column distinguishes fake news and real news. Label zero indicates fake news, and label 1 indicates real news. The label column has a binary data type (0 or 1).\n",
    "\n",
    "This project uses a statistical (machine learning) approach over a rule-based approach to scale the solution. Predicting the status of unseen news or information is the major goal of the model. The labels in the dataset help us to train and fit a model and then calculate the accuracy using test data. The accuracy of the prediction over unseen data other than this dataset will determine how useful the model is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Evaluation methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate our machine learning or deep learning models to know their performance. A good-performing model can be used to solve a machine learning or deep learning problem. Evaluation of a model also gives an idea about improving the performance of the model or selecting a better model based on the problem. We use the following methodologies to evaluate the text classification models used in this report. We use scikit learn's matrics class and its methods find the following values.\n",
    "\n",
    "Terms used below:<br>\n",
    "TP - true positive, FP - false positive, TN - true negative, FN - false negative\n",
    "\n",
    "**Accuracy** \n",
    "\n",
    "Accuracy is calculated by dividing the total true predictions by all predictions in the given dataset.\n",
    "\n",
    "The accuracy score is calculated by following formula. \n",
    "\n",
    "(TP + TN) / (TP + FP + TN + FN)\n",
    "\n",
    "Code: <br>\n",
    "from sklearn.metrics import accuracy_score<br>\n",
    "accuracy = accuracy_score(test_predictions, train_predictions)\n",
    "\n",
    "**Precision**\n",
    "\n",
    "The precision score is the measure of the model's ability to not label a negative sample as positive.\n",
    "\n",
    "TP / (TP + FP)\n",
    "\n",
    "Code:<br>\n",
    "from sklearn.metrics import precision_score<br>\n",
    "precision = precision_score(test_predictions, train_predictions)\n",
    "\n",
    "**Recall**\n",
    "\n",
    "The recall is the ability of the model to find all the positive values from the dataset.\n",
    "\n",
    "TP / (TP + FN)\n",
    "\n",
    "Code:<br>\n",
    "from sklearn.metrics import recall_score<br>\n",
    "recall = recall_score(test_predictions, train_predictions)\n",
    "\n",
    "**F1 Score**\n",
    "\n",
    "F1 score is the harmonic mean of precision and recall. The value of the F1 score is from 0 to 1. The values of precision and recall equally affect the F1 score.\n",
    "\n",
    "F1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "Code:<br>\n",
    "from sklearn.metrics import f1_score<br>\n",
    "f_score = f1_score(test_predictions, train_predictions)\n",
    "\n",
    "**Confusion matrix**\n",
    "\n",
    "Representation of TP, FP, FN, and TN as a matrix. We can plot the confusion matrix using scikit learn's ConfusionMatrixDisplay method and matplotlib library.\n",
    "\n",
    "Code:\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay<br>\n",
    "cmatrics = confusion_matrix(test_predictions, train_predictions)\n",
    "view = ConfusionMatrixDisplay(confusion_matrix=cmatrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we preprocess the data - a very important procedure in data science. It is cleaning the data to improve its quality without losing essential features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libraries\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read the CSV data into a python data frame and perform our operations on the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the csv data and store it in a dataframe\n",
    "news_data = pd.read_csv('WELFake_Dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore a few rows of the dataframe to see how the data is arranged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>LAW ENFORCEMENT ON HIGH ALERT Following Threat...</td>\n",
       "      <td>No comment is expected from Barack Obama Membe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Did they post their votes for Hillary already?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>UNBELIEVABLE! OBAMA’S ATTORNEY GENERAL SAYS MO...</td>\n",
       "      <td>Now, most of the demonstrators gathered last ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Bobby Jindal, raised Hindu, uses story of Chri...</td>\n",
       "      <td>A dozen politically active pastors came here f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>SATAN 2: Russia unvelis an image of its terrif...</td>\n",
       "      <td>The RS-28 Sarmat missile, dubbed Satan 2, will...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title  \\\n",
       "0           0  LAW ENFORCEMENT ON HIGH ALERT Following Threat...   \n",
       "1           1                                                NaN   \n",
       "2           2  UNBELIEVABLE! OBAMA’S ATTORNEY GENERAL SAYS MO...   \n",
       "3           3  Bobby Jindal, raised Hindu, uses story of Chri...   \n",
       "4           4  SATAN 2: Russia unvelis an image of its terrif...   \n",
       "\n",
       "                                                text  label  \n",
       "0  No comment is expected from Barack Obama Membe...      1  \n",
       "1     Did they post their votes for Hillary already?      1  \n",
       "2   Now, most of the demonstrators gathered last ...      1  \n",
       "3  A dozen politically active pastors came here f...      0  \n",
       "4  The RS-28 Sarmat missile, dubbed Satan 2, will...      1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the first five rows\n",
    "news_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72134, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prints the shape - (row, column)\n",
    "news_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in the 'title' and 'label' columns of the dataset. Let's check the basic statistical data of these columns using python's describe() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count                                                 71576\n",
       "unique                                                62347\n",
       "top       Factbox: Trump fills top jobs for his administ...\n",
       "freq                                                     14\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the statistical data of title column\n",
    "news_data['title'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    72134.000000\n",
       "mean         0.514404\n",
       "std          0.499796\n",
       "min          0.000000\n",
       "25%          0.000000\n",
       "50%          1.000000\n",
       "75%          1.000000\n",
       "max          1.000000\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the statistical data of label column\n",
    "news_data['label'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show there are 62347 unique titles out of 71576. So, there are duplicate titles in the dataset. We also need to check if there are any null values in the title column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove duplicate and null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for null values in the title column\n",
    "news_data['title'].isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>43</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True. Hillary needs a distraction and what bet...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>162</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All eyes on Electoral delegates. The People kn...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>185</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cool</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>269</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A leading US senator: US Supporting War in Syr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>318</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Federally fund democracy by federally funding ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0 title                                               text  \\\n",
       "43           43   NaN  True. Hillary needs a distraction and what bet...   \n",
       "162         162   NaN  All eyes on Electoral delegates. The People kn...   \n",
       "185         185   NaN                                               Cool   \n",
       "269         269   NaN  A leading US senator: US Supporting War in Syr...   \n",
       "318         318   NaN  Federally fund democracy by federally funding ...   \n",
       "\n",
       "     label  \n",
       "43       1  \n",
       "162      1  \n",
       "185      1  \n",
       "269      1  \n",
       "318      1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print a few duplicate title entries\n",
    "news_data[news_data['title'].duplicated()].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's drop all the duplicate title values and store the results in a new data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all the duplicate titles \n",
    "updated_data = news_data.drop_duplicates(subset='title', keep=False, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53998, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the new shape\n",
    "updated_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>LAW ENFORCEMENT ON HIGH ALERT Following Threat...</td>\n",
       "      <td>No comment is expected from Barack Obama Membe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>UNBELIEVABLE! OBAMA’S ATTORNEY GENERAL SAYS MO...</td>\n",
       "      <td>Now, most of the demonstrators gathered last ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Bobby Jindal, raised Hindu, uses story of Chri...</td>\n",
       "      <td>A dozen politically active pastors came here f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>DR BEN CARSON TARGETED BY THE IRS: “I never ha...</td>\n",
       "      <td>DR. BEN CARSON TELLS THE STORY OF WHAT HAPPENE...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>Latest Pipeline Leak Underscores Dangers Of Da...</td>\n",
       "      <td>FILE – In this Sept. 15, 2005 file photo, the ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title  \\\n",
       "0           0  LAW ENFORCEMENT ON HIGH ALERT Following Threat...   \n",
       "2           2  UNBELIEVABLE! OBAMA’S ATTORNEY GENERAL SAYS MO...   \n",
       "3           3  Bobby Jindal, raised Hindu, uses story of Chri...   \n",
       "6           6  DR BEN CARSON TARGETED BY THE IRS: “I never ha...   \n",
       "9           9  Latest Pipeline Leak Underscores Dangers Of Da...   \n",
       "\n",
       "                                                text  label  \n",
       "0  No comment is expected from Barack Obama Membe...      1  \n",
       "2   Now, most of the demonstrators gathered last ...      1  \n",
       "3  A dozen politically active pastors came here f...      0  \n",
       "6  DR. BEN CARSON TELLS THE STORY OF WHAT HAPPENE...      1  \n",
       "9  FILE – In this Sept. 15, 2005 file photo, the ...      1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the new dataframe\n",
    "updated_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice that the indexing of the new data frame is not continuous. Let's reset the index and then remove unnecessary columns including the 'text' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the index inplace\n",
    "updated_data.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>LAW ENFORCEMENT ON HIGH ALERT Following Threat...</td>\n",
       "      <td>No comment is expected from Barack Obama Membe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>UNBELIEVABLE! OBAMA’S ATTORNEY GENERAL SAYS MO...</td>\n",
       "      <td>Now, most of the demonstrators gathered last ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Bobby Jindal, raised Hindu, uses story of Chri...</td>\n",
       "      <td>A dozen politically active pastors came here f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>DR BEN CARSON TARGETED BY THE IRS: “I never ha...</td>\n",
       "      <td>DR. BEN CARSON TELLS THE STORY OF WHAT HAPPENE...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>Latest Pipeline Leak Underscores Dangers Of Da...</td>\n",
       "      <td>FILE – In this Sept. 15, 2005 file photo, the ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  Unnamed: 0                                              title  \\\n",
       "0      0           0  LAW ENFORCEMENT ON HIGH ALERT Following Threat...   \n",
       "1      2           2  UNBELIEVABLE! OBAMA’S ATTORNEY GENERAL SAYS MO...   \n",
       "2      3           3  Bobby Jindal, raised Hindu, uses story of Chri...   \n",
       "3      6           6  DR BEN CARSON TARGETED BY THE IRS: “I never ha...   \n",
       "4      9           9  Latest Pipeline Leak Underscores Dangers Of Da...   \n",
       "\n",
       "                                                text  label  \n",
       "0  No comment is expected from Barack Obama Membe...      1  \n",
       "1   Now, most of the demonstrators gathered last ...      1  \n",
       "2  A dozen politically active pastors came here f...      0  \n",
       "3  DR. BEN CARSON TELLS THE STORY OF WHAT HAPPENE...      1  \n",
       "4  FILE – In this Sept. 15, 2005 file photo, the ...      1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the dataframe\n",
    "updated_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove unnecessary columns \n",
    "updated_data = updated_data.drop(updated_data.columns[[0, 1, 3]], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LAW ENFORCEMENT ON HIGH ALERT Following Threat...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UNBELIEVABLE! OBAMA’S ATTORNEY GENERAL SAYS MO...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bobby Jindal, raised Hindu, uses story of Chri...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DR BEN CARSON TARGETED BY THE IRS: “I never ha...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Latest Pipeline Leak Underscores Dangers Of Da...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  label\n",
       "0  LAW ENFORCEMENT ON HIGH ALERT Following Threat...      1\n",
       "1  UNBELIEVABLE! OBAMA’S ATTORNEY GENERAL SAYS MO...      1\n",
       "2  Bobby Jindal, raised Hindu, uses story of Chri...      0\n",
       "3  DR BEN CARSON TARGETED BY THE IRS: “I never ha...      1\n",
       "4  Latest Pipeline Leak Underscores Dangers Of Da...      1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chekc the dataset\n",
    "updated_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title    False\n",
       "label    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if there is any null values remaining in the dataset\n",
    "updated_data.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # check if there is any duplicate values remaining in titles\n",
    "updated_data['title'].duplicated().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we got a satisfactory data frame without any duplicates or null values. Let's do further analysis on this data frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove special chars, tokenization and stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the title and label to features_x and label_y variables\n",
    "features_x = updated_data['title'].copy()\n",
    "label_y = updated_data['label'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to prepare the data to apply machine learning algorithms. The frequency of each word appearing in the dataset is important for vectorizing the words. The special characters, punctuations and most common words (the, and, a, is ) are not required. There are different forms for a word in languages, for example, running, runs, and ran are based on the root word run. We should map all the words to their root words by applying a stemming algorithm.\n",
    "\n",
    "We use the following methods to make the dataset machine-learning friendly.<br>\n",
    " 1. Remove punctuation and special characters using regular expressions.<br>\n",
    " 2. Split each news titles into words using a tokenizer.<br>\n",
    " 3. Remove common words using python stopwords.<br>\n",
    " 4. Apply nltk's porter stemmer algorithm to find the root words.<br>\n",
    " 5. Join the processed words for each news and add it to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import regular expression, nltk, tokenizer, stopwords and stemmer libraries\n",
    "import re\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For faster operation, we use the first 2000 rows from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare the porterstemmer\n",
    "pstemmer = PorterStemmer()\n",
    "# list to store processed information\n",
    "clean_data = []\n",
    "\n",
    "# process each row of the dataframe\n",
    "for row in updated_data['title'][:2000]:\n",
    "    # keep only letters and digits\n",
    "    sentance = re.sub('[^a-zA-Z0-9]', ' ', row)\n",
    "    # convert to lower case and tokenize\n",
    "    words = nltk.word_tokenize(sentance.lower())\n",
    "    # remove stopwords and stem words\n",
    "    clean_words = [pstemmer.stem(word) for word in words if not word in stopwords.words()]\n",
    "    # join tokenized words to form clean sentance\n",
    "    clean_sentance = ' '.join(clean_words)\n",
    "    # store cleaned data\n",
    "    clean_data.append(clean_sentance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check a few items in the list 'cleaned_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['law enforc high alert follow threat cop white 9 11bi blacklivesmatt fyf911 terrorist video',\n",
       " 'unbeliev obama attorney gener say charlott rioter peac protest home state north carolina video',\n",
       " 'bobbi jindal rais hindu use stori christian convers woo evangel potenti 2016 bid',\n",
       " 'dr carson target ir never audit spoke nation prayer breakfast',\n",
       " 'latest pipelin leak underscor danger dakota access pipelin']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the data is processed for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we will convert the clean news data to a matrix of numbers based on word frequency. We use scikit-learn's well-known text feature extraction method 'CountVectorizer' to perform this task.\n",
    "\n",
    "We will divide the dataset into a small set of 500 rows and another set of 2000 rows for better analysis based on the data size. Unigrams and bigrams will be extracted using the parameter 'ngram_range'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following code is based on the example given in scikit-learn.org\n",
    "# Link: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "# ngram_range: word n-grams - (1,2) unigram and bigrams\n",
    "count_vect = CountVectorizer(ngram_range = (1,2))\n",
    "# save the features and label\n",
    "# small dataset\n",
    "X_small = count_vect.fit_transform(clean_data[:500])\n",
    "y_small = updated_data['label'][:500]\n",
    "# large dataset\n",
    "X_large = count_vect.fit_transform(clean_data)\n",
    "y_large = updated_data['label'][:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns will represent the count of each word in a sentence, and the rows will represent the instances of news title data. This is also known as the **'bag of words'**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the vectorized features\n",
    "X_large.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'analyzer': 'word',\n",
       " 'binary': False,\n",
       " 'decode_error': 'strict',\n",
       " 'dtype': numpy.int64,\n",
       " 'encoding': 'utf-8',\n",
       " 'input': 'content',\n",
       " 'lowercase': True,\n",
       " 'max_df': 1.0,\n",
       " 'max_features': None,\n",
       " 'min_df': 1,\n",
       " 'ngram_range': (1, 2),\n",
       " 'preprocessor': None,\n",
       " 'stop_words': None,\n",
       " 'strip_accents': None,\n",
       " 'token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'tokenizer': None,\n",
       " 'vocabulary': None}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the parameters of the CountVectorizer\n",
    "count_vect.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Baseline performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measuring the baseline performance will give us a very basic idea about the performance of the model. As we deal with word frequencies, let's choose the **Multinomial Naive Bayes algorithm as the baseline** [8]. \n",
    "\n",
    "**Why Multinomial Naive Bayes as baseline?**\n",
    "\n",
    "It is based on probability and very easy to implement. Useful for quickly making predictions and is suitable for word counts based text classification [9]. This algorithm assumes all features are independent. So, the prediction accuracy can be lower, and we have a scope for improving the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into train and test data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit learns train_test_split method will split the data into random subsets for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import train_test_split\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data reference - [7]\n",
    "# The size of the test set is 20%\n",
    "# random_state parameter do the data shuffling over multiple calls\n",
    "# small data\n",
    "X_train_small, X_test_small, y_train_small, y_test_small = train_test_split(X_small, y_small, test_size=0.20, random_state=42)\n",
    "# large data\n",
    "X_train_large, X_test_large, y_train_large, y_test_large = train_test_split(X_large, y_large, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the Multinomial Naive Bayes using scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the libraries\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes classifier for multinomial model [9]\n",
    "# small dataset\n",
    "mnb_classifier_small = MultinomialNB()\n",
    "# fit the model on training data\n",
    "mnb_classifier_small.fit(X_train_small.toarray(), y_train_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# large dataset\n",
    "# Naive Bayes classifier for multinomial model [9]\n",
    "mnb_classifier_large = MultinomialNB()\n",
    "# fit the model on training data\n",
    "mnb_classifier_large.fit(X_train_large.toarray(), y_train_large)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the prediction on unseen data (test data) using the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted labels\n",
    "predicted_y_small = mnb_classifier_small.predict(X_test_small.toarray())\n",
    "predicted_y_large = mnb_classifier_large.predict(X_test_large.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the accuracy score, precision score, recall score and f1 score [10] to evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the metrics library\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small dataset\n",
    "# accuracy\n",
    "acc_score_small = metrics.accuracy_score(y_test_small,predicted_y_small)\n",
    "# here the targets are binary\n",
    "# precsion tp / (tp + fp)\n",
    "prec_score_small = metrics.precision_score(y_test_small,predicted_y_small,average='binary')\n",
    "# recall - tp / (tp + fn)\n",
    "rec_score_small = metrics.recall_score(y_test_small,predicted_y_small,average='binary')\n",
    "# f1 score - harmonic mean of precision and recall\n",
    "f1_score_small = metrics.f1_score(y_test_small,predicted_y_small,average='binary')\n",
    "\n",
    "# Large dataset - repeat the same\n",
    "acc_score_large = metrics.accuracy_score(y_test_large,predicted_y_large)\n",
    "prec_score_large = metrics.precision_score(y_test_large,predicted_y_large,average='binary')\n",
    "rec_score_large = metrics.recall_score(y_test_large,predicted_y_large,average='binary')\n",
    "f1_score_large = metrics.f1_score(y_test_large,predicted_y_large,average='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMALL DATASET - BASELINE\n",
      "Accuracy 0.8200\n",
      "F1 Score 0.7353\n",
      "Precision Score 0.6579\n",
      "Recall score 0.8333\n",
      "\n",
      "LARGE DATASET - BASELINE\n",
      "Accuracy 0.8400\n",
      "F1 Score 0.7987\n",
      "Precision Score 0.7651\n",
      "Recall score 0.8355\n"
     ]
    }
   ],
   "source": [
    "# Print the results\n",
    "print(\"SMALL DATASET - BASELINE\")\n",
    "print(\"Accuracy {0:.4f}\".format(acc_score_small))\n",
    "print(\"F1 Score {0:.4f}\".format(f1_score_small))\n",
    "print(\"Precision Score {0:.4f}\".format(prec_score_small))\n",
    "print(\"Recall score {0:.4f}\".format(rec_score_small))\n",
    "\n",
    "print(\"\\nLARGE DATASET - BASELINE\")\n",
    "print(\"Accuracy {0:.4f}\".format(acc_score_large))\n",
    "print(\"F1 Score {0:.4f}\".format(f1_score_large))\n",
    "print(\"Precision Score {0:.4f}\".format(prec_score_large))\n",
    "print(\"Recall score {0:.4f}\".format(rec_score_large))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the confusion matrix (large dataset) to display the performance of the classifier using matplotlib library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the libraries \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix [11]\n",
    "cmatrix = confusion_matrix(y_test_large, predicted_y_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATgAAAEGCAYAAADxD4m3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAb3ElEQVR4nO3de7xVZb3v8c+XxU0uIneRS0AbLbCkQvKSbm8l2kVtd4HK4/ZylLZWe+uuo90sfblP55hWu1LDy9FO5qXEsp2KRheyNAQ0BAlBJUAQ5CYICKy1fvuPMZZOca255ljMyZxz8H2/XuO15njmmM/4sRbrt55nPOMZjyICM7M86lTtAMzMKsUJzsxyywnOzHLLCc7McssJzsxyq3O1Ayg0oF9DjBzepdphWAZLFvWpdgiWwfbGzexs3q49qePk43vG+g1NJR07d/6OGRExaU/OtydqKsGNHN6F2TOGVzsMy+CDh59a7RAsgz+vuXOP61i/oYnZM0aUdGzDkCUD9viEe6CmEpyZ1b4AmmmudhglcYIzs0yCYFeU1kWtNic4M8vMLTgzy6UgaKqTKZ5OcGaWWTNOcGaWQwE01UmC842+ZpZZM1HSVoyk4ZJ+J2mRpIWSvpCW95P0sKQl6de+BZ+5TNJSSYslndxenE5wZpZJALsiStra0QhcEhFvB44ALpQ0FrgUmBkRY4CZ6T7pe5OBccAk4DpJDcVO4ARnZpkEQVOJW9F6IlZHxLz09RZgETAUOA24LT3sNuD09PVpwJ0RsSMingeWAhOLncPX4Mwsm4Cm0i/BDZA0p2B/WkRM2/0gSSOBdwF/AQZHxGpIkqCkQelhQ4HHCj62Mi1rkxOcmWWSzGQo2bqImFDsAEm9gHuAf42IzVKbU2Vbe6NoqnWCM7OMRFOruaYDNUldSJLb7RExPS1eI2lI2nobAqxNy1cChZPVhwGritXva3BmlkkyyKCStmKUNNVuBhZFxLUFb90HnJW+Pgv4ZUH5ZEndJI0CxgCzi53DLTgzyyS5D64sLbijgTOBpyQ9mZZ9GfgWcLekc4HlwMcBImKhpLuBp0lGYC+MKD4p1gnOzDJrbqd1VoqIeITWr6sBnNjGZ64Crir1HE5wZpZJGVtwFecEZ2aZBKKpTi7fO8GZWWbl6KLuDU5wZpZJIHZG0RlSNcMJzswySW70dRfVzHLKgwxmlksRoincgjOznGp2C87M8igZZKiP1FEfUZpZzfAgg5nlWpPvgzOzPPJMBjPLtWaPoppZHiWT7Z3gzCyHArHLU7XMLI8i8I2+ZpZX8o2+ZpZPgVtwZpZjHmQws1wK5Ademlk+JcsGlid1SLoF+BCwNiIOTcvuAg5JDzkA2BQR4yWNBBYBi9P3HouIqcXqd4Izs4zKt/AzcCvwA+DHLQUR8cnXziRdA7xccPyzETG+1Mqd4Mwsk6B8MxkiYlbaMnuTdGHoTwAndLT++rhSaGY1pSltxbW37aFjgDURsaSgbJSkJyT9QdIx7VXgFpyZZRKhLC24AZLmFOxPi4hpJX52CnBHwf5qYERErJf0HuAXksZFxOa2KnCCM7NMkkGGkqdqrYuICVnPIakz8FHgPa+dN2IHsCN9PVfSs8DBwJxWK8EJzswy2ytrMpwE/C0iVr52VmkgsCEimiSNBsYAzxWrxNfgzCyTZJBBJW3tkXQH8ChwiKSVks5N35rMG7unAMcC8yX9Ffg5MDUiNhSr3y04M8usXDMZImJKG+X/3ErZPcA9Wep3gjOzTDyTwcxyzYvOmFkuRcCuZic4M8uhpIvqBGdmOVXGuagV5QS3h9a+0IWrvzCCjWu7oE7BqZ9ZzxnnrWPzxgb+Y+pI1qzsyuBhO/nKj5bR+4Amdu0U3/vSMJbM74E6wWeveIHDjnql2v+MfVaXrk38n2l/oUuXZho6B3+aeSC3TxvDqDGbufDShezXo5E1q/fj6q8dxvatXaodbk1ouU2kHlQ0wUmaBHwPaABuiohvVfJ81dDQOTj/66sY887tbHulExdNOph3H7uFh+/qx7vet4VPfm4td31/EHf9YBDnfXU1D9zeH4Af/XYxm9Z15iufHs33H3iGTvXR4s+dXTs78eXPTuTV7Z1paGjm6pseY86fBzD1i4u4+XuHsGBef97/4RX805nP85MbDq52uDWifrqoFYtSUgPwQ+AUYCwwRdLYSp2vWvoPbmTMO7cD0KNXM8P/YQfrVnfh0Rl9OOkTyT2IJ31iA48+2AeA5c90413HJC22AwY00qtPE8/8tUd1gjdAvLo9+TvfuXPQ0DkgxLARr7BgXj8Anpg9gKOPf7GaQdac5nRdhva2aqtkGp4ILI2I5yJiJ3AncFoFz1d1L67oyrML9uNt797GxnVd6D+4EUiS4Kb1yS/R6HGv8uiMPjQ1wovLu7Jkfg9eWuWuTzV16hR8//ZHuP2hmTz5l/4sXngAf3+uN0ccuxaA9534IgMGv1rlKGtHMoraUNJWbZXsog4FVhTsrwTeu/tBks4HzgcYMbR+Lwlu39qJK88bydQrXqBn7+Y2jzt58nqWL+nGRZMOYdCwnYydsJWGhtiLkdrumpvF5z79Pnr22sVXr57HW966he9e8Q4u+PenmXLeUh6bNYjGXfXRJdsbfKNvorXvwJt+k9NHp0wDmHBY97r8TW/cBVeeN5ITPrqR952aPHy074BdrF/Tmf6DG1m/pjMH9E9acw2dYeo3V7322X/98BiGjt5Rlbjtjba+0oX5c/vxniNfYvpPRvO1z00E4KARWzn8fS9VObraUgvdz1JU8s/SSmB4wf4wYFUbx9atCLj2khEMH7ODf7rg9V+CIz6wmd/cnVzD+c3d/Tjy5CTxvbpNvLot+bbP/UMvGjoHbznYCa5a9j9gBz177QKga7cmxk9cz4plvejTN/mZSMHkc5bywD3Di1WzTynnZPtKq2QL7nFgjKRRwAskTwf4VAXPVxULZ/dk5s/7Mert2/nsSck6GWdftopPXrSGq6aO5ME7+zNoaHKbCMCm9V34ypTRqBP0P3AXX/r+36sYvfUbsIOLvzGfTp1AnYJHfnMgjz8yiI9MXsaHPpb8bP78+wN5+FfDqhxpbamXUVRFVK5XKOlU4Lskt4ncEhFXFTt+wmHdY/YM/6WsJx88/NRqh2AZ/HnNnby8c80eNa36vm1QnHDLx0o6dvrR18/tyAMvy6WiV/Uj4n7g/kqew8z2vlrofpaifoctzawqPJPBzHLNCc7Mcsn3wZlZrtXLfXBOcGaWSQQ0+oGXZpZX9dJFrY80bGY1o+UaXJmWDbxF0lpJCwrKviHpBUlPptupBe9dJmmppMWSTm6vfic4M8ssQiVtJbgVmNRK+XciYny63Q+QPm5tMjAu/cx16WPZ2uQEZ2aZlet5cBExCyi6eHOB04A7I2JHRDwPLCV5LFubnODMLJOITJPtB0iaU7CdX+JpLpI0P+3C9k3LWnsE29BilXiQwcwyEk2lj6Ku68Bc1OuBK0kmTVwJXAOcQ4mPYCvkBGdmmZV4fa2DdcealteSbgT+K93N/Ag2d1HNLJNKPw9O0pCC3TOAlhHW+4DJkrqlj2EbA8wuVpdbcGaWTSTX4cpB0h3AcSTX6lYClwPHSRqfnIllwAUAEbFQ0t3A00AjcGFENBWr3wnOzDIr11StiJjSSvHNRY6/Cij6XMlCTnBmlklkG2SoKic4M8usgg8CLysnODPLrJKjqOXkBGdmmUQ4wZlZjtXL00Sc4MwsM1+DM7NcCkSzR1HNLK/qpAHnBGdmGXmQwcxyrU6acE5wZpZZ3bfgJH2fInk6Ij5fkYjMrKYF0Nxc5wkOmLPXojCz+hFAvbfgIuK2wn1JPSNia+VDMrNaVy/3wbV7M4ukIyU9DSxK9w+TdF3FIzOz2hUlblVWyt163wVOBtYDRMRfgWMrGJOZ1bTSlgyshYGIkkZRI2KF9IZgiz5F08xyrgZaZ6UoJcGtkHQUEJK6Ap8n7a6a2T4oIOpkFLWULupU4EKS9QdfAMan+2a2z1KJW3W124KLiHXAp/dCLGZWL+qki1rKKOpoSb+S9JKktZJ+KWn03gjOzGpUmUZR05Xr10paUFB2taS/pSvb3yvpgLR8pKTtkp5Mtxvaq7+ULupPgbuBIcBBwM+AO0r4nJnlUcuNvqVs7bsVmLRb2cPAoRHxTuAZ4LKC956NiPHpNrW9yktJcIqI/x8Rjen2E+qmgWpmlRBR2tZ+PTEL2LBb2UMR0ZjuPkaygn2HtJngJPWT1A/4naRL0+bhWyR9Cfh1R09oZjnQrNK2PXcO8EDB/ihJT0j6g6Rj2vtwsUGGuSQttZYoLyh4L4Ars0ZqZvmg0vtwAyQVzmufFhHTSjqH9BWSFexvT4tWAyMiYr2k9wC/kDQuIja3VUexuaijSgnCzPYx2aZhrYuICVlPIeks4EPAiRFJZzcidgA70tdzJT0LHEyRB4OUNJNB0qHAWKB7S1lE/Dhr0GaWByUPIHSsdmkS8L+Af4yIbQXlA4ENEdGU3skxBniuWF3tJjhJlwPHkSS4+4FTgEcAJzizfVWZhhkl3UGSXwZIWglcTjJq2g14OJ0i+lg6YnoscIWkRpLpolMjYkOrFadKacF9DDgMeCIizpY0GLipg/8eM8uD5vJUExFTWim+uY1j7wHuyVJ/KQlue0Q0S2qUtD+wFvCNvmb7qjw88LLAnPRO4htJRlZfAWZXMigzq20ZRlGrqpS5qP+SvrxB0oPA/hExv7JhmVlNq/cEJ+ndxd6LiHmVCcnMrDyKteCuKfJeACeUORaemd+Dkw8aX+5qrYKeuXFotUOwDF69sktZ6qn7LmpEHL83AzGzOhGUaxpWxXnhZzPLrt5bcGZmban7LqqZWZvqJMGV8kRfSfqMpK+n+yMkTax8aGZWs3K0Lup1wJFAy5SKLcAPKxaRmdU0RelbtZXSRX1vRLxb0hMAEbExXT7QzPZVORpF3SWpgbTBmT6ypExTbc2sHtVC66wUpXRR/xO4Fxgk6SqSRyX9R0WjMrPaVifX4EqZi3q7pLnAiSSPLz89Iryyvdm+qkaur5WilAdejgC2Ab8qLIuI5ZUMzMxqWF4SHMkKWi2Lz3QHRgGLgXEVjMvMapjq5Cp8KV3UdxTup08ZuaCNw83MakbmmQwRMU/S4ZUIxszqRF66qJIuLtjtBLwbeKliEZlZbcvTIAPQu+B1I8k1uUwLP5hZzuQhwaU3+PaKiC/upXjMrB6Ub9nAW0gWeF4bEYemZf2Au4CRwDLgExGxMX3vMuBckmUDPx8RM4rV3+aNvpI6R0QTSZfUzAxIbqdQc2lbCW4FJu1WdikwMyLGADPTfSSNBSaT3MExCbgubYS1qdhMhpaVs56UdJ+kMyV9tGUrKXQzy58yTraPiFnA7os3nwbclr6+DTi9oPzOiNgREc8DS4GiTzYq5RpcP2A9yRoMLffDBTC9hM+aWR6V3kUdIGlOwf60iJjWzmcGR8RqgIhYLWlQWj4UeKzguJVpWZuKJbhB6QjqAl5PbC3q5BKjmVVE6RlgXURMKNNZW3uESdFIiiW4BqBXRyo1s3yr8G0iayQNSVtvQ4C1aflKYHjBccOAVcUqKpbgVkfEFXsWp5nlUmUT3H3AWcC30q+/LCj/qaRrgYOAMbw+VtCqYgmuPp5oZ2Z7V5RvLqqkO4DjSK7VrQQuJ0lsd0s6F1gOfBwgIhZKuht4muSe3AvTOz3aVCzBnbjn4ZtZLpWpBRcRU9p4q9X8ExFXAVeVWn+xhZ93H7o1MwPyNVXLzOyNnODMLJdq5HHkpXCCM7NMhLuoZpZjTnBmll9OcGaWW05wZpZLOXuir5nZGznBmVle5WbZQDOz3bmLamb55Bt9zSzXnODMLI88k8HMck3N9ZHhnODMLBtfgzOzPHMX1czyywnOzPLKLTgzyy8nODPLpTKuqlVpTnBmlkm57oOTdAhwV0HRaODrwAHA/wReSsu/HBH3d+QcTnBmll3seYaLiMXAeABJDcALwL3A2cB3IuLbe3oOJzgzy6wCgwwnAs9GxN+l8q057wRXRgMP2skXv7ecvoMaiWa4/yf9+cXNA/nMJS9yyqfW8/KG5Nv9//73EB7/7f5VjnbfNfjW5+k5fxNNvbvw928eCsCAn62g1/xNRIPYNbAbL549iuYenen92Hr6zlj92me7vbCd5V8dx44RPaoVfvVlu9F3gKQ5BfvTImJaK8dNBu4o2L9I0v8A5gCXRMTGjoRasQQn6RbgQ8DaiDi0UuepJU2NYtoVB7H0qR7s17OJHzz4DPNm9Qbg3hsH8vMbBlU5QgPYfNQANh0/iANvef61sm1j92fdR4dBgxjw8xX0u3816z42nC1H9GfLEf0B6LpyGwf9cOm+ndxSGQYZ1kXEhKJ1SV2BjwCXpUXXA1eSpNErgWuAczoSZ6eOfKhEtwKTKlh/zdmwtgtLn0r+82/f2sCKpd0ZMGRXlaOy3W0/uDdNPd/4t33buD7QkHSNXh3di84bd77pc71nb2DLxH57JcZap+bSthKdAsyLiDUAEbEmIpoiohm4EZjY0TgrluAiYhawoVL117rBw3by1kO387d5ScL78NnruP43i7n42uX06tNY5eismP3/9BJb39HnTeW95zjBAWkXNUrbSjOFgu6ppCEF750BLOhoqJVswZVE0vmS5kias4sd1Q6nLLr3aOJrNy3jhq8fxLZXGviv2/pz9pFv51/efzAb1nTh/MtXVTtEa0O/X6+CTmLLe/u/obz7c68QXTuxc6i7p5AMMpSytVuP1AN4PzC9oPj/SnpK0nzgeODfOhpn1QcZ0guO0wD2V786uT+6bQ2dg6/dtIzfTu/Lnx44AIBN67q89v4Dt/fnih8/38anrZr2//M6es7fxMqLD4HdRvJ6P76BLYe79faaMv2mRsQ2oP9uZWeWp/YaaMHlS3DxNStYsaQ706cNfK2036DXr8MddcrLLFvcvRrBWRE9FrxM3wdXs+qiMUS3hje+2Rz0cvf0NS03+pajBVdpVW/B5cm4iVs56eMbee7p7lz38GIguSXkuNM38dZx24mANSu78p9fGlblSPdtB057lh7PbKHhlUZGffFJ1n9kKP0eWI0amxl6bfJze3V0L9aeORKA/ZZsobFvV3YN9B8mACL8wEtJdwDHkdwHsxK4PCJurtT5asHC2b04+aDD3lTue95qy4vnv/VNZZuPGdjKkYnth+zPii+PrWRI9ac+8lvlElxETKlU3WZWXbXQ/SyFu6hmlk0A+3oX1cxyrD7ymxOcmWXnLqqZ5dY+P4pqZjnlZQPNLK+SG33rI8M5wZlZdl6Twczyyi04M8snX4Mzs/zyXFQzyzN3Uc0sl7zws5nlmltwZpZb9ZHfnODMLDs110cf1QnOzLIJfKOvmeWTiLLd6CtpGbAFaAIaI2KCpH7AXcBIYBnwiY6ubO9FZ8wsu/Kui3p8RIyPiAnp/qXAzIgYA8xM9zvECc7MsitvgtvdacBt6evbgNM7WpETnJll03INrpQtWXRqTsF2fiu1PSRpbsF7gyNiNUD6dVBHQ/U1ODPLLMMo6rqCrmdrjo6IVZIGAQ9L+tueR/c6t+DMLKMSu6cldFEjYlX6dS1wLzARWCNpCED6dW1HI3WCM7NsgrIkOEk9JfVueQ18AFgA3AeclR52FvDLjobqLqqZZVee++AGA/dKgiQX/TQiHpT0OHC3pHOB5cDHO3oCJzgzy6wc98FFxHPAYa2UrwdO3OMT4ARnZh3hyfZmlksR0FQfc7Wc4MwsO7fgzCy3nODMLJcC8JoMZpZPAeFrcGaWR4EHGcwsx3wNzsxyywnOzPJpj571tlc5wZlZNgF40Rkzyy234MwsnzxVy8zyKiB8H5yZ5ZZnMphZbvkanJnlUoRHUc0sx9yCM7N8CqKpqdpBlMQJzsyy8eOSzCzX6uQ2Ea+LamaZBBDNUdJWjKThkn4naZGkhZK+kJZ/Q9ILkp5Mt1M7GqtbcGaWTZTtgZeNwCURMS9dAHqupIfT974TEd/e0xM4wZlZZuUYZIiI1cDq9PUWSYuAoXtccQFFDQ33SnoJ+Hu146iAAcC6agdhmeT1Z/aWiBi4JxVIepDk+1OK7sCrBfvTImJaK3WOBGYBhwIXA/8MbAbmkLTyNnYo1lpKcHklaU5ETKh2HFY6/8z2Hkm9gD8AV0XEdEmDSf64BHAlMCQizulI3R5kMLOqkdQFuAe4PSKmA0TEmohoimRG/43AxI7W7wRnZlUhScDNwKKIuLagfEjBYWcACzp6Dg8y7B1vuuZgNc8/s8o7GjgTeErSk2nZl4EpksaTdFGXARd09AS+BmdmueUuqpnllhOcmeWWE1wFSZokabGkpZIurXY81j5Jt0haK6nDF7atdjjBVYikBuCHwCnAWJILp2OrG5WV4FZgUrWDsPJwgqucicDSiHguInYCdwKnVTkma0dEzAI2VDsOKw8nuMoZCqwo2F9JmefZmVlxTnCVo1bKfE+O2V7kBFc5K4HhBfvDgFVVisVsn+QEVzmPA2MkjZLUFZgM3FflmMz2KU5wFRIRjcBFwAxgEXB3RCysblTWHkl3AI8Ch0haKencasdkHeepWmaWW27BmVluOcGZWW45wZlZbjnBmVluOcGZWW45wdURSU3pQrgLJP1MUo89qOtWSR9LX99U7EEAko6TdFQHzrFM0ptWX2qrfLdjXsl4rm9I+vesMVq+OcHVl+0RMT4iDgV2AlML30yfYJJZRJwXEU8XOeQ4IHOCM6s2J7j69UfgH9LW1e8k/ZTk2fYNkq6W9Lik+ZIugGSBD0k/kPS0pF8Dg1oqkvR7SRPS15MkzZP0V0kz0/UqpwL/lrYej5E0UNI96Tkel3R0+tn+kh6S9ISkH9H6fNw3kPQLSXMlLZR0/m7vXZPGMlPSwLTsrZIeTD/zR0lvK8t303LJi87UIUmdSZ4z92BaNBE4NCKeT5PEyxFxuKRuwJ8kPQS8CzgEeAcwGHgauGW3egeSLNN2bFpXv4jYIOkG4JWI+HZ63E+B70TEI5JGkMzWeDtwOfBIRFwh6YPAGxJWG85Jz7Ef8LikeyJiPdATmBcRl0j6elr3RSSLwUyNiCWS3gtcB5zQgW+j7QOc4OrLfgWrD/2RZMm1o4DZEfF8Wv4B4J0t19eAPsAY4FjgjohoAlZJ+m0r9R8BzGqpKyLaei7aScDYZNU3APaX1Ds9x0fTz/5aUimrkX9e0hnp6+FprOuBZuCutPwnwPR0geCjgJ8VnLtbCeewfZQTXH3ZHhHjCwvSX/SthUXA5yJixm7HnUr7j2tSCcdAcmnjyIjY3kosJc/9k3QcSbI8MiK2Sfo90L2NwyM976bdvwdmbfE1uPyZAXw2XTEcSQdL6gnMAian1+iGAMe38tlHgX+UNCr9bL+0fAvQu+C4h0i6i6THjU9fzgI+nZadAvRtJ9Y+wMY0ub2NpAXZohPQ0gr9FEnXdzPwvKSPp+eQpMPaOYftw5zg8ucmkutr89KFU35E0lK/F1gCPAVcD/xh9w9GxEsk182mS/orr3cRfwWc0TLIAHwemJAOYjzN66O53wSOlTSPpKu8vJ1YHwQ6S5oPXAk8VvDeVmCcpLkk19iuSMs/DZybxrcQPwbeivDTRMwst9yCM7PccoIzs9xygjOz3HKCM7PccoIzs9xygjOz3HKCM7Pc+m9OnOGiAX7h8gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot confusion matrix [9]\n",
    "view = ConfusionMatrixDisplay(confusion_matrix=cmatrix, display_labels=mnb_classifier_large.classes_)\n",
    "view.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Classification approach\n",
    "\n",
    "In this section, we are building our classifier to work on the binary text classification (fake news or real news) dataset. The goal of the classifier is to produce a better result than the baseline classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features and labels\n",
    "\n",
    "Our data frame is clean and has two columns (title and label). The title column contains the news titles and the label column stores whether it is fake or not. Label 1 stands for the real news and 0 for fake news. We chose the title column as the feature (X) and the label column as the label of the dataset (y). These are already stored in variables X, and y respectively in the feature extraction step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Labels 0    1\n",
      "1    1\n",
      "2    0\n",
      "3    1\n",
      "4    1\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Print a few features and it's labels (large dataset)\n",
    "print(\"Features:\", X_large.toarray())\n",
    "print(\"Labels\", y_large[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose the classifier\n",
    "\n",
    "Logistic regression, K-nearest neighbours, Support vector machines, Decision trees, and Naive Bayes are all well-known text classification algorithms. Support Vector Machine (SVM) is one of the popular models and is prefered by many for its accuracy and speed. Let's choose SVM and compare the performance against our baseline model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperpaprameter turning\n",
    "\n",
    "Hyperparameters are the parameters set to the model before the training process [12]. Hyperparameter tuning is the process of optimizing these parameters for the algorithm like running time and accuracy. These parameters control the important properties of the algorithm. Better optimized hyperparameters can improve the performance of the model. For support vector machines, C, and gamma are usually optimized hyperparameters.\n",
    "\n",
    "We use scikit-learn libraries GridSearchCV method to find the best hyperparameters.[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svc model object\n",
    "svc_model = SVC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the scikit-learn libraries GridSearchCV [13] method to find the best hyperparameters. GridSearchCV uses the following parameter grid to generate candidates for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid parameters - code reference [13]\n",
    "# Link: https://scikit-learn.org/stable/modules/grid_search.html\n",
    "grid_parameters = [\n",
    "  {'C': [0.1, 1, 10, 100, 1000], 'gamma': [1, 0.1, 0.01, 0.001, 0.0001], 'kernel': ['rbf']},\n",
    "  {'C': [0.1, 1, 10, 100, 1000], 'kernel': ['linear']},\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearchCV code reference [14]\n",
    "# Link: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "gridsearch_cv = GridSearchCV(svc_model, grid_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model with small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=SVC(),\n",
       "             param_grid=[{'C': [0.1, 1, 10, 100, 1000],\n",
       "                          'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
       "                          'kernel': ['rbf']},\n",
       "                         {'C': [0.1, 1, 10, 100, 1000], 'kernel': ['linear']}])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model with best hyperparameters\n",
    "gridsearch_cv.fit(X_train_small, y_train_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "#  the best parameters identified [14]\n",
    "print(gridsearch_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=100, gamma=0.001)\n"
     ]
    }
   ],
   "source": [
    "# model with hyper params [14]\n",
    "print(gridsearch_cv.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate the predictions and use scikit learn's classification_report method to show the main classification matrices including the class-wise scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.96      0.91        70\n",
      "           1       0.86      0.63      0.73        30\n",
      "\n",
      "    accuracy                           0.86       100\n",
      "   macro avg       0.86      0.80      0.82       100\n",
      "weighted avg       0.86      0.86      0.85       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate the predictions after hyperparameter tuning\n",
    "grid_predictions = gridsearch_cv.predict(X_test_small) \n",
    "# print classification report\n",
    "print(classification_report(y_test_small, grid_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_acc_score = metrics.accuracy_score(y_test_small,grid_predictions)\n",
    "# here the targets are binary\n",
    "svm_f1_score = metrics.f1_score(y_test_small,grid_predictions)\n",
    "svm_prec_score = metrics.precision_score(y_test_small,grid_predictions)\n",
    "svm_rec_score = metrics.recall_score(y_test_small,grid_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMALL DATASET - MAIN CLASSIFIER\n",
      "Accuracy 0.8600\n",
      "F1 Score 0.7308\n",
      "Precision Score 0.8636\n",
      "Recall score 0.6333\n"
     ]
    }
   ],
   "source": [
    "print(\"SMALL DATASET - MAIN CLASSIFIER\")\n",
    "print(\"Accuracy {0:.4f}\".format(svm_acc_score))\n",
    "print(\"F1 Score {0:.4f}\".format(svm_f1_score))\n",
    "print(\"Precision Score {0:.4f}\".format(svm_prec_score))\n",
    "print(\"Recall score {0:.4f}\".format(svm_rec_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model with large dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=SVC(),\n",
       "             param_grid=[{'C': [0.1, 1, 10, 100, 1000],\n",
       "                          'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
       "                          'kernel': ['rbf']},\n",
       "                         {'C': [0.1, 1, 10, 100, 1000], 'kernel': ['linear']}])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearch_cv.fit(X_train_large, y_train_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1, 'kernel': 'linear'}\n",
      "SVC(C=1, kernel='linear')\n"
     ]
    }
   ],
   "source": [
    "#  the best parameters identified [14]\n",
    "print(gridsearch_cv.best_params_)\n",
    "# model with hyper params [14]\n",
    "print(gridsearch_cv.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As performed above, We capture the predictions and show the main classification matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.88      0.88       248\n",
      "           1       0.81      0.79      0.80       152\n",
      "\n",
      "    accuracy                           0.85       400\n",
      "   macro avg       0.84      0.84      0.84       400\n",
      "weighted avg       0.85      0.85      0.85       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate the predictions after hyperparameter tuning\n",
    "grid_predictions = gridsearch_cv.predict(X_test_large) \n",
    "# show classification report\n",
    "print(classification_report(y_test_large, grid_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LARGE DATASET - MAIN CLASSIFIER\n",
      "Accuracy 0.8475\n",
      "F1 Score 0.7973\n",
      "Precision Score 0.8054\n",
      "Recall score 0.7895\n"
     ]
    }
   ],
   "source": [
    "svm_acc_score = metrics.accuracy_score(y_test_large,grid_predictions)\n",
    "# here the targets are binary\n",
    "svm_f1_score = metrics.f1_score(y_test_large,grid_predictions)\n",
    "svm_prec_score = metrics.precision_score(y_test_large,grid_predictions)\n",
    "svm_rec_score = metrics.recall_score(y_test_large,grid_predictions)\n",
    "\n",
    "print(\"LARGE DATASET - MAIN CLASSIFIER\")\n",
    "print(\"Accuracy {0:.4f}\".format(svm_acc_score))\n",
    "print(\"F1 Score {0:.4f}\".format(svm_f1_score))\n",
    "print(\"Precision Score {0:.4f}\".format(svm_prec_score))\n",
    "print(\"Recall score {0:.4f}\".format(svm_rec_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# confusion matrix [11]\n",
    "cmatrix_svm = confusion_matrix(y_test_large, grid_predictions)\n",
    "\n",
    "# plot confusion matrix [9]\n",
    "view_svm = ConfusionMatrixDisplay(confusion_matrix=cmatrix_svm, display_labels=gridsearch_cv.classes_)\n",
    "view_svm.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can deep learning improve it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning models gained a great reputation among researchers for text classification. Here we try to implement a well-known recurrent neural network, LSTM. LSTM stands for Long-Short Term Memory, we use TensorFlow and Keras to implement LSTM. The following cells of code is based on the example code provided by tensorflow.org [15]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tensorflow and numpy libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the larger dataset into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test split\n",
    "X_train_tf, X_test_tf, y_train_tf, y_test_tf = train_test_split(clean_data, y_large, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a new text encoder using Keras' TextVectorization. It converts the features to numbers using a preprocessing layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following cells of code is based on the example given in tensorflow.org - Reference [15]\n",
    "#Link: https://www.tensorflow.org/text/tutorials/text_classification_rnn\n",
    "# word size\n",
    "VOCABULARY_SIZE = 60000\n",
    "word_encoder = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=VOCABULARY_SIZE)\n",
    "# sets the layer's vocabulary\n",
    "word_encoder.adapt(X_train_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we group the linear stack of layers to Keras' sequential model. The sequential model is used to create the LSTM deep learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code reference [15]\n",
    "# Build the LSTM model using Keras Sequential model\n",
    "lstm_model = tf.keras.Sequential([\n",
    "    word_encoder,\n",
    "    #convert integers into vectors using the Embedding class\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(word_encoder.get_vocabulary()),\n",
    "        output_dim=64,\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)), # Bidirectional wrapper for LSTM\n",
    "    tf.keras.layers.Dense(64, activation='relu'), # Layer activation using ReLU function\n",
    "    tf.keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, we compile the training configurations (loss, optimizer and matrices) using the compile method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply training configurations to the model\n",
    "lstm_model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), # loss function\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4), # apply adam optimizer algorithm\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model by adding the training data and epoch (number of times it goes through the training data), test data, and steps per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "50/50 [==============================] - 18s 70ms/step - loss: 0.6881 - accuracy: 0.6137 - val_loss: 0.6833 - val_accuracy: 0.6200\n",
      "Epoch 2/10\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.6749 - accuracy: 0.6137 - val_loss: 0.6672 - val_accuracy: 0.6200\n",
      "Epoch 3/10\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.6503 - accuracy: 0.6137 - val_loss: 0.6370 - val_accuracy: 0.6200\n",
      "Epoch 4/10\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.5910 - accuracy: 0.6137 - val_loss: 0.5701 - val_accuracy: 0.6200\n",
      "Epoch 5/10\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.4651 - accuracy: 0.6162 - val_loss: 0.4655 - val_accuracy: 0.6275\n",
      "Epoch 6/10\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.3119 - accuracy: 0.8019 - val_loss: 0.3949 - val_accuracy: 0.8300\n",
      "Epoch 7/10\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.1811 - accuracy: 0.9456 - val_loss: 0.3651 - val_accuracy: 0.8450\n",
      "Epoch 8/10\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.1091 - accuracy: 0.9719 - val_loss: 0.3952 - val_accuracy: 0.8375\n",
      "Epoch 9/10\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.0738 - accuracy: 0.9800 - val_loss: 0.4506 - val_accuracy: 0.8325\n",
      "Epoch 10/10\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.0541 - accuracy: 0.9850 - val_loss: 0.4992 - val_accuracy: 0.8375\n"
     ]
    }
   ],
   "source": [
    "# training the model, epoch = 10\n",
    "train_lstm = lstm_model.fit(np.asarray(X_train_tf), np.asarray(y_train_tf), epochs=10,\n",
    "                    validation_data=(np.asarray(X_test_tf), np.asarray(y_test_tf)),\n",
    "                    validation_steps=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the accuracy of the deep learning model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 2s 5ms/step - loss: 0.4992 - accuracy: 0.8375\n",
      "LSTM RNN: Test Loss: 0.4992479085922241\n",
      "LSTM RNN: Test Accuracy: 0.8374999761581421\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test data\n",
    "lstm_test_loss, lstm_test_accuracy = lstm_model.evaluate(np.asarray(X_test_tf), np.asarray(y_test_tf))\n",
    "\n",
    "print('LSTM RNN: Test Loss:', lstm_test_loss)\n",
    "print('LSTM RNN: Test Accuracy:', lstm_test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the performance of the models in the following dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have chosen four evaluation matrices for each model implemented. Accuracy, precision, recall, and F1 Score are calculated above. Let's compare those matrices here and analyse the performance of each model. We use the evaluation matrices mentioned above to analyse the performance of each model (accuracy, precision, recall, F1 score).\n",
    "\n",
    "\n",
    "### Baseline vs Main Classifier\n",
    "\n",
    "**Baseline: Multinomial Naive Bayes, Dataset size: 500**<br> \n",
    "Accuracy 0.8200<br>\n",
    "F1 Score 0.7353<br>\n",
    "Precision Score 0.6579<br>\n",
    "Recall score 0.8333\n",
    "\n",
    "**Classifier: SVM, Dataset size: 500**<br>\n",
    "Accuracy 0.8600<br>\n",
    "F1 Score 0.7308<br>\n",
    "Precision Score 0.8636<br>\n",
    "Recall score 0.6333\n",
    "\n",
    "**Baseline: Multinomial Naive Bayes, Dataset size: 2000**<br> \n",
    "Accuracy 0.8400<br>\n",
    "F1 Score 0.7987<br>\n",
    "Precision Score 0.7651<br>\n",
    "Recall score 0.8355\n",
    "\n",
    "**Classifier: SVM, Dataset size: 2000**<br>\n",
    "Accuracy 0.8475<br>\n",
    "F1 Score 0.7973<br>\n",
    "Precision Score 0.8054<br>\n",
    "Recall score 0.7895\n",
    "\n",
    "We first used a smaller dataset (500 rows from CSV) to train the model, increased its size to 2000 (keeping the first 500) and observed the results. Our main classifier outperformed the baseline classifier for the smaller dataset. But, when the size increased, the accuracy of the SVM dropped a little bit (from 0.86 to 0.8475), and the baseline's accuracy increased (from 0.82 - 0.84). The baseline classifier improved all the matrices on the larger dataset. Both baseline and main classifiers showed a solid performance, and almost equal accuracy on the larger dataset.\n",
    "\n",
    "**Deep Learning: LSTM**<br>\n",
    "\n",
    "LSTM RNN: Accuracy: 0.8375\n",
    "\n",
    "Since 2010 deep learning has gained a reputation due to the high availability of computational capacity (GPU, tensors etc). Our LSTM RNN model didn't outperform the main classifier on a larger dataset. But it showed almost the same accuracy (0.8375)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Summary and conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The execution of the models on different sizes of datasets and their evaluation leads us to the following conclusions. It is based on the evidence we observed in the baseline, classification, and evaluation steps.\n",
    "\n",
    "1. Simple text classification models like Multinomial Naive Bayes may provide better results with larger datasets.\n",
    "\n",
    "2. Popular machine learning models like SVM are better for smaller datasets, and their performance may drop as data size increases.\n",
    "\n",
    "3. The main classifier (SVM) outperformed the baseline classifier on the smaller dataset with 500 samples.\n",
    "\n",
    "4. Our baseline classifier Multinomial Naive Bayes showed almost the same performance as the main classifier (SVM) on the larger dataset with 2000 samples.\n",
    "\n",
    "5. SVM is widely regarded as one of the best text classification algorithms. But simple methods like Multinomial Naive Bayes may provide the same performance. It is always good to compare the performance of your model with a simple model. If a simple and faster model can provide the same performance, we can save time and computational power.\n",
    "\n",
    "6. The deep learning model (LSTM) showed almost the same accuracy as the main classifier and there is still room for improvement.\n",
    "\n",
    "\n",
    "### Can a deep learning model provide a better result in text classification?\n",
    "\n",
    "The deep learning model (LSTM) showed a very solid performance on the large dataset. Though it took some time to train the model, the accuracy of the model (0.8375) matches our main classifier. Due to the limitation in computing capacity, I was unable to process all the samples in the dataset (sample size 62347). The first 2000 samples were used to train and test the LSTM model. This limitation caused to fully evaluate the performance of the LSTM model. But the results showed almost equal accuracy compared to the main classifier. Based on the evidence, we can say that this investigation showed comparable results between the deep learning and machine learning models, and the LSTM model didn't provide a better result.\n",
    "\n",
    "### Limitations and Future Work\n",
    "\n",
    "We identified a well-balanced dataset from Kaggle but were unable to process all the sample sizes due to the lack of computational resources. We couldn't evaluate the performance of the LSTM deep learning model. \n",
    "\n",
    "The objective of this report was to provide proof-of-concept of fake news detection solutions for the public. The models discussed above can act as a backend service for an end-user application in the future. The future works include fully evaluating our models using all the data. We will better optimize the deep learning model to improve accuracy, share the code in GitHub repository, and deploy the text classifier solution as a backend service to our software (Mobile app and web app)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] https://www.statista.com/statistics/617136/digital-population-worldwide/\n",
    "\n",
    "[2] https://www.politico.eu/article/cambridge-analytica-chris-wylie-brexit-trump-britain-data-protection-privacy-facebook/\n",
    "\n",
    "[3] https://www.nytimes.com/2020/07/28/us/politics/russia-disinformation-coronavirus.html\n",
    "\n",
    "[4] https://www.theguardian.com/technology/2021/oct/22/twitter-admits-bias-in-algorithm-for-rightwing-politicians-and-news-outlets\n",
    "\n",
    "[5] https://www.independent.co.uk/tech/elon-musk-twitter-deal-bias-b2100553.html\n",
    "\n",
    "[6] https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "\n",
    "[7] https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "\n",
    "[8] https://scikit-learn.org/stable/modules/naive_bayes.html\n",
    "\n",
    "[9] https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html\n",
    "\n",
    "[10] https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score\n",
    "\n",
    "[11] https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html\n",
    "\n",
    "[12] https://en.wikipedia.org/wiki/Hyperparameter_optimization\n",
    "\n",
    "[13] https://scikit-learn.org/stable/modules/grid_search.html\n",
    "\n",
    "[14] https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "\n",
    "[15] https://www.tensorflow.org/text/tutorials/text_classification_rnn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
